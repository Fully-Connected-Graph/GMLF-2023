{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://fully-connected-graph.github.io/GMLF-2023/assets/banner.png)\n",
    "\n",
    "# Introduction to Recommender Systems\n",
    "\n",
    "This is practical session is part of the first lecture of the [Machine Learning Fortnight](https://mlfortnight.svcover.nl/) organized by the [Fully Connected Graph](https://svcover.nl/committees?commissie=programming_committee).\n",
    "\n",
    "Alongside the lectures there is a competition on Kaggle, register for it to copmete with others and test your knowledge.\n",
    "\n",
    "## Movie Recommendation Practical Session\n",
    "\n",
    "In this practical session you will get hands-on practice in building recommendation systems. The dataset we are exploring is the MovieLens 100K movie ratings. It is made up of user-item pairs as well as relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the necessary libraries\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the nessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import faiss\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# download the dataset if not already downloaded\n",
    "if not os.path.exists('ml-100k'):\n",
    "    !wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "    !unzip ml-100k.zip\n",
    "    !rm ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set consists of:\n",
    "\t\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "* Each user has rated at least 20 movies. \n",
    "    * Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "Let's see the structure of the dataset, there are 3 important files:\n",
    "- u.data: contains the edges/reviews from each user to a movie.\n",
    "- u.user: details of a user.\n",
    "- u.item: details of a movie.\n",
    "\n",
    "Aside from these are auxiliary files which add context to the dataset.\n",
    "It's a good idea to check the README file to understand the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's examine each of the important files and process them into a more usable format. We will first look at the data file which contains the reviews of each user for a movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rating data into a dataframe\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    'ml-100k/u1.base', sep='\\t', \n",
    "    names=['user_id', 'movie_id', 'rating', 'timestamp']\n",
    ")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genres data into a dataframe\n",
    "\n",
    "genre_data = pd.read_csv(\n",
    "    'ml-100k/u.genre', sep='|', \n",
    "    names=['genre', 'genre_id']\n",
    ")\n",
    "genre_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie data into a dataframe\n",
    "\n",
    "movie_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url'] + genre_data['genre'].tolist()\n",
    "\n",
    "movie_data = pd.read_csv(\n",
    "    'ml-100k/u.item', sep='|', \n",
    "    names=movie_cols,\n",
    "    encoding='latin-1'\n",
    ")\n",
    "\n",
    "# Make movie_id the index\n",
    "movie_data.set_index('movie_id', inplace=True)\n",
    "\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user data into a dataframe\n",
    "\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "\n",
    "user_data = pd.read_csv(\n",
    "    'ml-100k/u.user', sep='|', \n",
    "    names=user_cols,\n",
    "    encoding='latin-1'\n",
    ")\n",
    "\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "Let's drop the irrelevant data, such as\n",
    "\n",
    "From `user_data` we will drop the following columns:\n",
    "- `zip code`\n",
    "    - Geographical location might be correlated with user preferences, but it is complex to process\n",
    "\n",
    "From `ratings`, we will drop the following columns\n",
    "- `timestamps` for user-movie pair reviews (u.data)\n",
    "    - time can be used as a feature, because user preferences change over time, but we will not use it in this practical session.m `movie_data` we will drop the following columns:\n",
    "- `movie title`\n",
    "    - we will use the movie id to identify movies\n",
    "- `release date`\n",
    "- `video release date`\n",
    "- `IMDb URL`\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "# Drop irrelevant columns\n",
    "user_data.drop('zip code', axis=1, inplace=True)\n",
    "ratings.drop('timestamp', axis=1, inplace=True)\n",
    "movie_data.drop(['movie title', 'release date', 'video release date', 'IMDb URL'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the columns to remove\n",
    "\n",
    "ratings.drop(columns=[\n",
    "    # YOUR CODE HERE\n",
    "], inplace=True)\n",
    "\n",
    "user_data.drop(columns=[\n",
    "    # YOUR CODE HERE\n",
    "], inplace=True)\n",
    "\n",
    "movie_data.drop(columns=[\n",
    "    # YOUR CODE HERE\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding of Categorical Data\n",
    "\n",
    "The user table contains information about their occupation and gender which is represented in text. \n",
    "It is necessary to convert this column into a more usable format\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "encoded_user_data = pd.get_dummies(encoded_user_data, columns=[\n",
    "    'gender', 'occupation'\n",
    "], drop_first=True, dtype='int')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_user_data = user_data.copy()\n",
    "\n",
    "# OHE the occupation and gender columns\n",
    "encoded_user_data = pd.get_dummies(encoded_user_data, columns=[\n",
    "    # YOUR CODE HERE\n",
    "], drop_first=True, dtype='int')\n",
    "encoded_user_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Numerical Data\n",
    "\n",
    "The age of users and the rating of movies are numerical data, but they are not on the same scale. We will also convert the ratings column into 0 and 1 to simplify training.\n",
    "\n",
    "You can use StandardScaler from sklearn to normalize the data.\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "age_scaler = StandardScaler()\n",
    "encoded_user_data['age'] = age_scaler.fit_transform(encoded_user_data[['age']])\n",
    "\n",
    "ratings['rating'] = ratings['rating'] / 5.0\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalize user age and rating\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Divide rating by 5\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data\n",
    "\n",
    "Our training data is obtained from u1.base which contains 80% of the data. We will use `u1.test` to test our model which contains the remaining 20%.\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "test_ratings.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "test_ratings['rating'] = test_ratings['rating'] / 5.0\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same process (of normalization) to the test data\n",
    "\n",
    "test_ratings = pd.read_csv(\n",
    "    'ml-100k/u1.test', sep='\\t', \n",
    "    names=['user_id', 'movie_id', 'rating', 'timestamp']\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "test_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Filtering\n",
    "\n",
    "In content based filtering, we will use the features of the items to recommend similar items to the user.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*Lr6qL0YjY_WqVK5u-AYHAQ.png\" width=\"360\" height=\"240\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different metrics that we can use to compute similarity between items. In order for these metrics to work, we can think of the tables as multidimensional vectors.\n",
    "\n",
    "### Euclidian Distance\n",
    "\n",
    "This considers the distance between vector heads. Accounts for magnitude and direction.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}     \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Dot Product\n",
    "\n",
    "Computes the products between components. Accounts for magnitude and direction.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n (x_i.y_i)  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "This metric considers the angle between the vectors. Accounts for direction only.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A useful library is FAISS (Facebook AI Similarity Search), which is a library for efficient similarity search and clustering of dense vectors. It contains implementations of the above metrics and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement K Nearest Neighbors\n",
    "\n",
    "The kNN algorithm can be used to find the k most similar items to a given item. You can read the documentation of FAISS [here](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexFlatL2.html)\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# Create index with idmap\n",
    "database = faiss.IndexIDMap(faiss.IndexFlatL2(movie_shape))\n",
    "database.add_with_ids(content_movie_data.values, content_movie_data.index.values)\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = movie_data[['title']]\n",
    "content_movie_data = movie_data.drop(columns=['title'])\n",
    "\n",
    "# Get shape of embeddings\n",
    "movie_shape = content_movie_data.shape[1]\n",
    "\n",
    "# TODO: Create a Database for the movies\n",
    "## YOUR CODE HERE\n",
    "database = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to debug the predictions\n",
    "def display_movie(movie_id, item_data):\n",
    "\n",
    "    # Get the movie title\n",
    "    movie_title = movie_titles.loc[movie_id]['title']\n",
    "    print(\"Movie title:\", movie_title)\n",
    "\n",
    "def display_predictions(item_data, movie_id, closest_movie_ids):\n",
    "\n",
    "    print(\"Original movie\")\n",
    "    display_movie(movie_id, item_data)\n",
    "\n",
    "    for index in range(len(closest_movie_ids)):\n",
    "        print(\"Recommendation\", index + 1)\n",
    "        display_movie(closest_movie_ids[index], item_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "_, closest_movie_ids = self.database.search(random_movie_dimensions.reshape(1, -1), k_neighbours)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentModel:\n",
    "\n",
    "    def __init__(self, train_data, item_data, database):\n",
    "        self.known_knowledge = train_data\n",
    "        self.item_data = item_data\n",
    "        self.database = database\n",
    "\n",
    "\n",
    "    # Select a random user, get a random movie they rated, and return a movie that is similar to that movie\n",
    "    def get_recommendation(self, user_id, rating, verbose=False, k_neighbours=3): \n",
    "\n",
    "        # If rating > 0.5, set ascending to False, else set it to True\n",
    "        order = rating >= 0.5\n",
    "\n",
    "        # Get the best or worst rated movie by the user\n",
    "        movie_id = self.known_knowledge.loc[(self.known_knowledge['user_id'] == user_id)].sort_values(by='rating', ascending=order).iloc[0]['movie_id'] \n",
    "        \n",
    "        # Get the dimensions of the movie\n",
    "        random_movie_dimensions = self.item_data.loc[movie_id].values\n",
    "\n",
    "        # TODO: Get the closest movie using faiss\n",
    "\n",
    "        # Get the closest movie using faiss, \n",
    "        # first var is distance, second is index\n",
    "        closest_movie_ids = ## YOURS CODE HERE\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            display_predictions(self.item_data, movie_id, closest_movie_ids[0])\n",
    "\n",
    "        # Calculate the mean of the ratings of the closest movies\n",
    "        mean = self.known_knowledge[self.known_knowledge['movie_id'].isin(closest_movie_ids[0])]['rating'].mean()\n",
    "\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_model = ContentModel(ratings, content_movie_data, database)\n",
    "\n",
    "# Get a random row from the training data\n",
    "random_row = test_ratings.sample(1)\n",
    "\n",
    "content_model.get_recommendation(random_row['user_id'].values[0], random_row['rating'].values[0], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score you get is the average rating of the k most similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on test data\n",
    "\n",
    "def evaluate_model(model, test_data):\n",
    "    # YOUR CODE HERE\n",
    "    predictions = []\n",
    "    for index, row in tqdm(test_data.iterrows(), total=test_data.shape[0]):\n",
    "        predictions.append(model.get_recommendation(row['user_id'], row['rating']))\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "predictions = evaluate_model(content_model, test_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Now that we have the predictions, we can use a metric to evaluate the model. Recall that our task is regression based\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "print(\"MAE:\", mae(test_ratings['rating'], predictions))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Calculate the Error\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Now that the data preprocessing part has been taken care of, let's get to the more exciting part, the algorithm. The algorithm that we'll introduce today is Matrix Factorization.\n",
    "\n",
    "Recall that we had a user-item matrix, $R$, where nonzero elements of the matrix are ratings that a user has given an item. What Matrix Factorization does is it decomposes a large matrix into products of matrices, namely, $R = U \\times V$. See the picture below taken from a quick google search for a better understanding:\n",
    "\n",
    "![R = U x V](https://ethen8181.github.io/machine-learning/recsys/img/matrix_factorization.png)\n",
    "\n",
    "Matrix factorization assumes that:\n",
    "- Each user can be described by $d$ features. For example, feature 1 might be a referring to how much each user likes disney movies.\n",
    "- Each item, movie in this case, can be described by an analogous set of $d$ features. To correspond to the above example, feature 1 for the movie might be a number that says how close the movie is to a disney movie.\n",
    "After learning the two smaller matrices, all we have to do is to perform a matrix multiplication of the two matrices and the end result will be a our approximation for the rating the user would give that item (movie).\n",
    "\n",
    "The cool thing about this is that, we do not know what these features are nor do we have to determine them beforehand, which is why these features are often refer to as latent features. Next, we also don't know how many latent features are optimal for the task at hand. Thus, we can use random search or grid search or other fancy techniques to perform hyperparameter tuning and determine the best number for $d$.\n",
    "\n",
    "Given all those information, the next question is: how do we learn the user matrix, $U$, and item matrix, $V$? Well, like a lot of machine learning algorithm, by minimizing a loss function.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "We start by denoting our $d$ feature user into math by letting a user $u$ take the form of a $1 \\times d$-dimensional vector $\\textbf{x}_{u}$. These for often times referred to as latent vectors or low-dimensional embeddings. Similarly, an item *i* can be represented by a $1 \\times d$-dimensional vector $\\textbf{y}_{i}$. And the rating that we predict user $u$ will give for item $i$ is just the dot product of the two vectors\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat r_{ui} &= \\textbf{x}_{u} \\textbf{y}_{i}^{T} = \\sum\\limits_{d} x_{ud}y_{di}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\hat r_{ui}$ represents our prediction for the true rating $r_{ui}$. Next, we will choose a objective function to minimize the square of the difference between all ratings in our dataset ($S$) and our predictions. This produces a objective function of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L &= \\sum\\limits_{u,i \\in S}( r_{ui} - \\textbf{x}_{u} \\textbf{y}_{i}^{T} )^{2} + \\lambda \\big( \\sum\\limits_{u} \\left\\Vert \\textbf{x}_{u} \\right\\Vert^{2} + \\sum\\limits_{i} \\left\\Vert \\textbf{y}_{i} \\right\\Vert^{2} \\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that we've added on two $L_{2}$ regularization terms, with $\\lambda$ controlling the strength at the end to prevent overfitting of the user and item vectors. $\\lambda$, is another hyperparameter that we'll have to search for to determine the best value. The concept of regularization can be a topic of itself, and if you're confused by this, consider checking out [this documentation](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/regularization/regularization.ipynb) that covers it a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithm\n",
    "\n",
    "Now that we formalize our objective function, we'll introduce the **Alternating Least Squares with Weighted Regularization (ALS-WR)** method for optimizing it. The way it works is we start by treating one set of latent vectors as constant. For this example, we'll pick the item vectors, $\\textbf{y}_{i}$. We then take the derivative of the loss function with respect to the other set of vectors, the user vectors, $\\textbf{x}_{u}$ and solve for the non-constant vectors (the user vectors).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\textbf{x}_{u}} \n",
    "&= - 2 \\sum\\limits_{i}(r_{ui} - \\textbf{x}_{u} \\textbf{y}_{i}^{T} ) \\textbf{y}_{i} + 2 \\lambda \\textbf{x}_{u} = 0 \\\\\n",
    "&= -(\\textbf{r}_{u} - \\textbf{x}_{u} Y^{T} )Y + \\lambda \\textbf{x}_{u} = 0 \\\\\n",
    "&= \\textbf{x}_{u} (Y^{T}Y + \\lambda I) = \\textbf{r}_{u}Y \\\\\n",
    "&= \\textbf{x}_{u} = \\textbf{r}_{u}Y (Y^{T}Y + \\lambda I)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To clarify it a bit, let us assume that we have $m$ users and $n$ items, so our ratings matrix is $m \\times n$.\n",
    "\n",
    "- The row vector $\\textbf{r}_{u}$ represents users *u*'s row from the ratings matrix with all the ratings for all the items (so it has dimension $1 \\times n$)\n",
    "-  We introduce the symbol $Y$, with dimensions $n \\times d$, to represent all item row vectors vertically stacked on each other\n",
    "- Lastly, $I$ is the identity matrix which has dimension $d \\times d$ to ensure the matrix multiplication's dimensionality will be correct when we add the $\\lambda$\n",
    "\n",
    "Now comes the alternating part: With these newly updated user vectors in hand, in the next round, we hold them as constant, and take the derivative of the loss function with respect to the previously constant vectors (the item vectors). As the derivation for the item vectors is quite similar, we will simply list out the end formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\textbf{y}_{i}}\n",
    "&= \\textbf{y}_{i} = \\textbf{r}_{i}X (X^{T}X + \\lambda I)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then we alternate back and forth and carry out this two-step process until convergence. The reason we alternate is, optimizing user latent vectors, $U$, and item latent vectors $V$ simultaneously is hard to solve. If we fix $U$ or $V$ and tackle one problem at a time, we potentially turn it into a easier sub-problem. Just to summarize it, ALS works by:\n",
    "\n",
    "1. Initialize the user latent vectors, $U$, and item latent vectors $V$ with randomly\n",
    "2. Fix $U$ and solve for $V$\n",
    "3. Fix $V$ and solve for $U$\n",
    "4. Repeat step 2 and 3 until convergence\n",
    "\n",
    "Now that we have our equations, let's program this thing up! We'll set the model to use 20 factors and a regularization value of 0.01 (chosen at random) and train it for 100 iterations and compute the mean square error on the train and test set to assess algorithm convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Let's first make the user and movies matricies and initialize them with random values.\n",
    "\n",
    "Use the [`np.random.normal()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) function to generate random values for the user and movie matrices.\n",
    "\n",
    "Use the scale parameter to set the standard deviation of the distribution to `1 / n_users` and `1 / n_movies` respectively.\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "user_matrix = np.random.normal(\n",
    "    scale=1./n_users, \n",
    "    size=(n_users, embed_dim)\n",
    ")\n",
    "item_matrix = np.random.normal(\n",
    "    scale=1./n_items, \n",
    "    size=(n_items, embed_dim)\n",
    ")\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "n_users = ratings.user_id.unique().shape[0]\n",
    "n_items = ratings.movie_id.unique().shape[0]\n",
    "\n",
    "user_matrix = np.random.normal( # YOUR CODE HERE\n",
    "    scale=1./n_users, \n",
    "    size=(n_users, embed_dim)\n",
    ")\n",
    "item_matrix = np.random.normal( # YOUR CODE HERE\n",
    "    scale=1./n_items, \n",
    "    size=(n_items, embed_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a helper predict function. It calculates the dot product of the user and movie matrices.\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "def predict(user_matrix, item_matrix):\n",
    "    \"\"\"predict ratings for every user and item\"\"\"\n",
    "    pred = user_matrix.dot(item_matrix.T) # TODO: YOUR CODE HERE\n",
    "    return pred\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(user_matrix, item_matrix):\n",
    "    \"\"\"predict ratings for every user and item\"\"\"\n",
    "    pred = user_matrix.dot(item_matrix.T) # TODO: YOUR CODE HERE\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a helper function to compute the mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def compute_mse(y_pred, y_true):\n",
    "    \"\"\"ignore zero terms prior to comparing the mse\"\"\"\n",
    "    mask = np.nonzero(y_true)\n",
    "    mse = mean_squared_error(y_true[mask], y_pred[mask])\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement one step of the ALS algorithm. Follow the steps from the formula above.\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "def als_step(ratings, solve_vecs, fixed_vecs, regulazation_lambda = 0.01):\n",
    "    \"\"\"\n",
    "    when updating the user matrix,\n",
    "    the item matrix is the fixed vector and vice versa\n",
    "    \"\"\"\n",
    "    A = fixed_vecs.T.dot(fixed_vecs) + np.eye(embed_dim) * regulazation_lambda\n",
    "    b = ratings.dot(fixed_vecs)\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    solve_vecs = b.dot(A_inv)\n",
    "    return solve_vecs\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_step(ratings, solve_vecs, fixed_vecs, regulazation_lambda = 0.01):\n",
    "    \"\"\"\n",
    "    when updating the user matrix,\n",
    "    the item matrix is the fixed vector and vice versa\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    A = fixed_vecs.T.dot(fixed_vecs) + np.eye(embed_dim) * regulazation_lambda\n",
    "    b = ratings.dot(fixed_vecs)\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    solve_vecs = b.dot(A_inv)\n",
    "    return solve_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the training data into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ratings.copy()\n",
    "train = train.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)\n",
    "\n",
    "test = test_ratings.copy()\n",
    "test = test.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pass in training and testing at the same time to record\n",
    "model convergence, assuming both dataset is in the form\n",
    "of User x Item matrix with cells as ratings\n",
    "\"\"\"\n",
    "\n",
    "n_iters = 100\n",
    "LAMBDA = 0.01\n",
    "\n",
    "# record the training and testing mse for every iteration\n",
    "# to show convergence later (usually, not worth it for production)\n",
    "train_mse_record = []\n",
    "test_mse_record  = []\n",
    "for _ in tqdm(range(n_iters)):\n",
    "    user_matrix = als_step(train.values, user_matrix, item_matrix, regulazation_lambda=LAMBDA)\n",
    "    item_matrix = als_step(train.values.T, item_matrix, user_matrix, regulazation_lambda=LAMBDA)\n",
    "    predictions = predict(user_matrix, item_matrix)\n",
    "    train_mse = compute_mse(train.values, predictions)\n",
    "\n",
    "    test_user_matrix = user_matrix[test.index.values - 1]\n",
    "    test_item_matrix = item_matrix[test.columns.values - 1]\n",
    "    test_predictions = predict(test_user_matrix, test_item_matrix)\n",
    "    test_mse = compute_mse(test.values, test_predictions)\n",
    "    \n",
    "    test_mse_record.append(test_mse)\n",
    "    train_mse_record.append(train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(n_iters), train_mse_record, label='Train', color='red')\n",
    "plt.plot(range(n_iters), test_mse_record, label='Test', color='blue')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ALS converges after a few sweeps, which is one of the main reason for its popularity. Fast, thus scalable to bigger dataset.\n",
    "\n",
    "Possibly your model has overfitted, to prevent this, you can play with the regularization parameter, embedding dimension, and number of iterations.\n",
    "\n",
    "<details>\n",
    "<summary>Answer to what lambda regularization parameter to choose</summary>\n",
    "\n",
    "From our experiments, we found that a value of from 20 to 50 work well.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate it with MAE\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "print(\"MAE:\", mae(test.values, test_predictions))\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the Error for the ALS model on the test data\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
