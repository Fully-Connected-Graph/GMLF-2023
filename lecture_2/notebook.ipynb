{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Recommender Systems\n",
    "\n",
    "Welcome to the second lecture of the Machine Learning Fortnight! It will be a more practical lecture, where you will get to see how to process the data, analyse it and apply recommedation systems. We will go through the collaborative filtering and content-based filtering techniques.\n",
    "\n",
    "You can copy this code to use for the competition. You can also use it as a reference for your future projects. By no means is this the only way to do it, but it is a good starting point.\n",
    "\n",
    "For this assignment, we will use the anime recommedation dataset from [Kaggle](https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020). The dataset contains information about anime, users and their ratings. The goal is to predict the rating that a user would give to an anime that they have not yet rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "!echo '{\"username\":\"username\",\"key\":\"key\"}' > ~/.kaggle/kaggle.json\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# download dataset\n",
    "!kaggle datasets download -d hernan4444/anime-recommendation-database-2020\n",
    "\n",
    "# create data folder\n",
    "!mkdir data\n",
    "\n",
    "# unzip dataset\n",
    "!unzip -q anime-recommendation-database-2020.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # pandas is a dataframe library\n",
    "import numpy as np                  # numpy provides N-dim object support\n",
    "import matplotlib.pyplot as plt     # matplotlib.pyplot plots data\n",
    "import seaborn as sns               # seaborn is the big brother of matplotlib\n",
    "import os                           # os handles directory/workspace changes\n",
    "from tqdm import tqdm_notebook      # tqdm_notebook is the progress bar library\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split out training and testing data\n",
    "from sklearn.preprocessing import LabelEncoder       # to convert labels into numbers\n",
    "from sklearn.metrics import mean_absolute_error      # for mean absolute error\n",
    "\n",
    "import tensorflow as tf             # tensorflow is the machine learning library we will be using\n",
    "\n",
    "\n",
    "# ––––––––––––––––––––––––\n",
    "#           Setup\n",
    "# ––––––––––––––––––––––––\n",
    "\n",
    "pd.set_option('display.max_columns', 100) # Display up to 100 columns of a dataframe\n",
    "pd.set_option('display.max_rows', 100)    # Display up to 100 rows of a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings File\n",
    "\n",
    "The `rating_complete.csv` file contains the ratings of the users. Each row is a pair of user and anime and rating (0-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"./data/rating_complete.csv\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rating.hist()\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's normalize the ratings (0, 1)\n",
    "ratings[\"rating\"] = ratings[\"rating\"] / 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's anaylse how many ratings have people given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.user_id.value_counts().hist()\n",
    "plt.title('Distribution of how many ratings each user has given')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Count of users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears someone has given out 160k ratings.\n",
    "\n",
    "Let's assume that an anime is 12 episodes long and each episode is 24 minutes long. Then, time taken to watch 160k anime is\n",
    "$160k * (24 * 12) / (60) = 768000$ hours of non-stop watch of anime. If this person daily watched 8 hours of anime non-stop, then it would take them 264 years. What is possible, but quite unlikely))).\n",
    "\n",
    "Reasonably, the maximum would be 10k hours of anime, which is 1 year and 2 months of non-stop anime watching.\n",
    "\n",
    "This is approximately $10k * 60 / (24 * 12) = 2084$ anime. So, let's remove the users who have rated more than 2084 animes. Most likely, they are bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of users who have rated more than 2084 anime\n",
    "bot_users = ratings.user_id.value_counts() > 2084\n",
    "bot_users = bot_users[bot_users == True].index\n",
    "\n",
    "# remove bot users from ratings dataframe\n",
    "ratings = ratings[~ratings.user_id.isin(bot_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.user_id.value_counts().hist()\n",
    "plt.title('Distribution of how many ratings each user has given')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Count of users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still crazy, but more reasonable. Weebs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also remove the users who have rated less than 40 animes. They are not weebs enough. And animes that have been rated less than 40 times. They are not popular enough.\n",
    "\n",
    "It would be difficult to work with such users and animes, because there is not enough data to make a good prediction.\n",
    "\n",
    "We will use the average anime rating for them as prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_size = ratings.shape[0]\n",
    "\n",
    "min_ratings = 40\n",
    "\n",
    "while True:\n",
    "    # get list of users who have rated less than 40 anime (min_ratings)\n",
    "    not_weeb_users = ratings.user_id.value_counts() < min_ratings\n",
    "    not_weeb_users = not_weeb_users[not_weeb_users == True].index\n",
    "\n",
    "    # get list of animes which have been rated less than 40 times (min_ratings)\n",
    "    not_popular_anime = ratings.anime_id.value_counts() < min_ratings\n",
    "    not_popular_anime = not_popular_anime[not_popular_anime == True].index\n",
    "\n",
    "    # remove them\n",
    "    ratings = ratings[~ratings.user_id.isin(not_weeb_users)]\n",
    "    ratings = ratings[~ratings.anime_id.isin(not_popular_anime)]\n",
    "\n",
    "    if ratings.shape[0] == prev_size:\n",
    "        break\n",
    "    else:\n",
    "        prev_size = ratings.shape[0]\n",
    "\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the user and anime ids to be from 0 to N-1, where N is the number of users and animes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_enc = LabelEncoder()\n",
    "ratings[\"user_enc_id\"] = user_enc.fit_transform(ratings[\"user_id\"].values)\n",
    "\n",
    "anime_enc = LabelEncoder()\n",
    "ratings[\"anime_enc_id\"] = anime_enc.fit_transform(ratings[\"anime_id\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anime File\n",
    "\n",
    "The `anime_with_synopsis.csv` file contains information about the anime. Each row is an anime and its information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv(\"./data/anime_with_synopsis.csv\")\n",
    "anime.columns = ['anime_id', 'name', 'average_rating', 'genres', 'synopsis']\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between animes (if there are animes not present in ratings)\n",
    "anime_not_present = len(set(ratings.anime_id.unique()) - set(anime.anime_id.unique()))\n",
    "\n",
    "print(\"There are {} animes present in ratings that are not present in anime\".format(anime_not_present))\n",
    "print(\"There are {} animes in anime_with_synopsis.csv\".format(anime.shape[0]))\n",
    "print(\"There are {} animes in rating_complete.csv\".format(ratings.anime_id.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the genres column into a list of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geners to dummy variables\n",
    "anime = anime.join(anime.genres.str.get_dummies(sep=','))\n",
    "\n",
    "# drop genres column\n",
    "anime.drop('genres', axis=1, inplace=True)\n",
    "\n",
    "# format the columns to lowercase and remove spaces\n",
    "anime.columns = anime.columns \\\n",
    "    .str.strip() \\\n",
    "    .str.lower() \\\n",
    "    .str.replace(' ', '_') \\\n",
    "    .str.replace('-', '_') \\\n",
    "    \n",
    "\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Let's split the data into train and test sets. We will use the train set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42, stratify=ratings['user_enc_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results table\n",
    "\n",
    "Let's create a table to store the results of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'test_MAE'])\n",
    "\n",
    "def add_res(model_name, test_mae):\n",
    "    global results\n",
    "    results = pd.concat([results, pd.DataFrame([[model_name, test_mae]], columns=['Model', 'test_MAE'])])\n",
    "    display(results.sort_values(by=['test_MAE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "There are some baseline models that we can implement which will give us a good starting point.\n",
    "\n",
    "### User Average\n",
    "\n",
    "The first baseline is the average rating of the user. It is the average rating that the user has given to all animes. If the user has not rated any animes, then we use the average rating of all users.\n",
    "\n",
    "### Anime Average\n",
    "\n",
    "The second baseline is the average rating of the anime. It is the average rating that the anime has received from all users. If the anime has not been rated by any user, then we use the average rating of all animes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a model to try user average rating\n",
    "\n",
    "class UserAverageRating:\n",
    "    def __init__(self):\n",
    "        self.user_averages = None\n",
    "        self.global_average = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_average = np.mean(y)\n",
    "\n",
    "        print(\"Calculating user averages...\")\n",
    "        self.user_averages = X.groupby('user_enc_id')['rating'].mean()\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Predicting...\")\n",
    "        y_pred = []\n",
    "        for user_id in tqdm_notebook(X.user_enc_id):\n",
    "            if user_id in self.user_averages.index:\n",
    "                y_pred.append(self.user_averages[user_id])\n",
    "            else:\n",
    "                y_pred.append(self.global_average)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "user_average_rating = UserAverageRating()\n",
    "user_average_rating.fit(train_ratings, train_ratings.rating)\n",
    "\n",
    "y_pred = user_average_rating.predict(test_ratings)\n",
    "\n",
    "mae = mean_absolute_error(test_ratings.rating, y_pred)\n",
    "\n",
    "add_res('User Average Rating', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeAverageRating:\n",
    "    def __init__(self):\n",
    "        self.anime_averages = None\n",
    "        self.global_average = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_average = np.mean(y)\n",
    "\n",
    "        print(\"Calculating anime averages...\")\n",
    "        self.anime_averages = X.groupby('anime_enc_id')['rating'].mean()\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Predicting...\")\n",
    "        y_pred = []\n",
    "        for anime_id in tqdm_notebook(X.anime_enc_id):\n",
    "            if anime_id in self.anime_averages.index:\n",
    "                y_pred.append(self.anime_averages[anime_id])\n",
    "            else:\n",
    "                y_pred.append(self.global_average)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "anime_average_rating = AnimeAverageRating()\n",
    "anime_average_rating.fit(train_ratings, train_ratings.rating)\n",
    "\n",
    "y_pred = anime_average_rating.predict(test_ratings)\n",
    "\n",
    "mae = mean_absolute_error(test_ratings.rating, y_pred)\n",
    "\n",
    "add_res('Anime Average Rating', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's pretty good. Let's try to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "For collaborative we will use matrix factorization, in TensorFlow with gradient descent.\n",
    "\n",
    "We will use the following formula for the prediction:\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\mu + b_u + b_i + q_i^Tp_u\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the average rating, $b_u$ is the user bias, $b_i$ is the anime bias, $q_i$ is the anime embedding and $p_u$ is the user embedding.\n",
    "\n",
    "We will use the following loss function:\n",
    "$$\n",
    "L = \\sum_{u,i} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda_1(||q_i||^2 + ||p_u||^2) + \\lambda_2(b_i^2 + b_u^2)\n",
    "$$\n",
    "\n",
    "where $\\lambda_1$, $\\lambda_2$ are the regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "class MatrixFactorizationModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, l2_emb, l2_bias):\n",
    "        super(MatrixFactorizationModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.user_emb = tf.keras.layers.Embedding(num_users, embedding_dim, embeddings_regularizer=l2(l2_emb))\n",
    "        self.item_emb = tf.keras.layers.Embedding(num_items, embedding_dim, embeddings_regularizer=l2(l2_emb))\n",
    "\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1, embeddings_regularizer=l2(l2_bias))\n",
    "        self.item_bias = tf.keras.layers.Embedding(num_items, 1, embeddings_regularizer=l2(l2_bias))\n",
    "\n",
    "        self.bias = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_ids = inputs[:, 0]\n",
    "        item_ids = inputs[:, 1]\n",
    "\n",
    "        user_emb = self.user_emb(user_ids)\n",
    "        item_emb = self.item_emb(item_ids)\n",
    "        user_bias = self.user_bias(user_ids)\n",
    "        item_bias = self.item_bias(item_ids)\n",
    "\n",
    "        user_emb = tf.reshape(user_emb, [-1, self.embedding_dim])\n",
    "        item_emb = tf.reshape(item_emb, [-1, self.embedding_dim])\n",
    "\n",
    "        rating_pred = tf.keras.layers.Dot(axes=1)([user_emb, item_emb])\n",
    "        rating_pred = tf.add(rating_pred, self.bias)\n",
    "        rating_pred = tf.add(rating_pred, user_bias)\n",
    "        rating_pred = tf.add(rating_pred, item_bias)\n",
    "\n",
    "        return rating_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings.user_enc_id.nunique()\n",
    "num_anime = ratings.anime_enc_id.nunique()\n",
    "\n",
    "mf_model = MatrixFactorizationModel(num_users = num_users, \n",
    "                                    num_items = num_anime, \n",
    "                                    embedding_dim = 64,\n",
    "                                    l2_emb  = 0.1,\n",
    "                                    l2_bias = 0.01)\n",
    "\n",
    "mf_model.compile(\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(),\n",
    "    loss = tf.keras.losses.MeanSquaredError(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.MeanAbsoluteError(\"MAE\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = mf_model.fit(\n",
    "    x = train_ratings[[\"user_enc_id\", \"anime_enc_id\"]].values,\n",
    "    y = train_ratings[\"rating\"].values,\n",
    "    batch_size = 500_000, # this is quite big, but it will run faster\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mf_model.predict(test_ratings[[\"user_enc_id\", \"anime_enc_id\"]].values)\n",
    "\n",
    "mae = mean_absolute_error(test_ratings.rating, y_pred)\n",
    "\n",
    "add_res('Matrix Factorization', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the model is not completely converged, might need to run longer to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares\n",
    "\n",
    "For collaborative we will use matrix factorization, in TensorFlow with alternating least squares. It is similar to gradient descent, but it is faster. It is also a bit more difficult to implement.\n",
    "\n",
    "We will use the following formula for the prediction:\n",
    "$$\n",
    "\\hat{r}_{u,i} = q_i^Tp_u\n",
    "$$\n",
    "\n",
    "where $q_i$ is the anime embedding and $p_u$ is the user embedding. We will not use the biases, because they are not needed.\n",
    "\n",
    "We will use the following loss function:\n",
    "$$\n",
    "L = \\sum_{u,i} (r_{u,i} - \\hat{r}_{u,i})^2.\n",
    "$$\n",
    "\n",
    "This can be optimized using alternating least squares. First, we optimize $p_u$ while keeping $q_i$ fixed. Then, we optimize $q_i$ while keeping $p_u$ fixed. We repeat this until convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationModel_ALS(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super(MatrixFactorizationModel_ALS, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "        self.user_emb = tf.keras.layers.Embedding(num_users, embedding_dim)\n",
    "        self.item_emb = tf.keras.layers.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_ids = inputs[:, 0]\n",
    "        item_ids = inputs[:, 1]\n",
    "\n",
    "        user_emb = self.user_emb(user_ids)\n",
    "        item_emb = self.item_emb(item_ids)\n",
    "        \n",
    "        user_emb = tf.reshape(user_emb, [-1, self.embedding_dim])\n",
    "        item_emb = tf.reshape(item_emb, [-1, self.embedding_dim])\n",
    "\n",
    "        rating_pred = tf.keras.layers.Dot(axes=1)([user_emb, item_emb])\n",
    "\n",
    "        return rating_pred\n",
    "    \n",
    "    def custom_fit(self, epochs, x, y):\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch {}/{} |\".format(epoch+1, epochs), end=\"\\t\")\n",
    "            self.user_emb.trainable = epoch % 2 == 0\n",
    "            self.item_emb.trainable = epoch % 2 == 1\n",
    "\n",
    "            self.fit(\n",
    "                x = x,\n",
    "                y = y,\n",
    "                batch_size = 500_000,\n",
    "                epochs = 1,\n",
    "                verbose = 1,\n",
    "                shuffle = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model = MatrixFactorizationModel_ALS(\n",
    "    num_users = num_users,\n",
    "    num_items = num_anime,\n",
    "    embedding_dim = 64)\n",
    "\n",
    "# compile the model\n",
    "als_model.compile(\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(),\n",
    "    loss = tf.keras.losses.MeanSquaredError(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.MeanAbsoluteError(\"MAE\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "als_model.custom_fit(\n",
    "    epochs = 10,\n",
    "    x = train_ratings[[\"user_enc_id\", \"anime_enc_id\"]].values,\n",
    "    y = train_ratings[\"rating\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = als_model.predict(test_ratings[[\"user_enc_id\", \"anime_enc_id\"]].values)\n",
    "\n",
    "mae = mean_absolute_error(test_ratings.rating, y_pred)\n",
    "\n",
    "add_res('Matrix Factorization ALS', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Filtering\n",
    "\n",
    "For content-based filtering we will use the genres of the anime, and BERT embeddings of the synopsis. We will use the embedding to find the similarity between the animes. Then, we will use the similarity to find the rating of the anime.\n",
    "\n",
    "We will use the following formula for the prediction:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\frac{\\sum_{j \\in N_i} s_{i,j} r_{u,j}}{\\sum_{j \\in N_i} s_{i,j}}\n",
    "$$\n",
    "\n",
    "where $N_i$ is the set of animes similar to anime $i$ and $s_{i,j}$ is the similarity between anime $i$ and anime $j$.\n",
    "\n",
    "---\n",
    "\n",
    "In the current implementation we will be using the following formula for the prediction. Not weighted by similarity.\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\frac{\\sum_{j \\in N_i} r_{u,j}}{|N_i|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres\n",
    "\n",
    "Let's make a model where each user is an average of the genres of the animes that they have rated. Then, we will use the genres of the anime to predict the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ––\n",
    "# THIS IS MOST INEFFICIENT WAY TO DO THIS\n",
    "# THINK MAYBE HOW TO PARALLELIZE THIS\n",
    "# BY CHANGING THE FOR LOOP TO LINEAR ALGEBRA\n",
    "# ––\n",
    "\n",
    "class GenresSimRating:\n",
    "    def __init__(self):\n",
    "        self.user_emb = {}\n",
    "        self.user_mean = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Calculating user embeddings...\")\n",
    "        self.user_mean = X.groupby('user_enc_id')['rating'].mean()\n",
    "        \n",
    "        for user_id, group in tqdm_notebook(X.groupby('user_enc_id')):\n",
    "            selection = anime.anime_id.isin(group.anime_id)\n",
    "            if selection.sum() == 0:\n",
    "                continue\n",
    "            emb = anime[selection].iloc[:, 4:].mean()\n",
    "            self.user_emb[user_id] = emb / np.linalg.norm(emb)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Predicting...\")\n",
    "        y_pred = []\n",
    "\n",
    "        for user_id, anime_id in tqdm_notebook(zip(X.user_enc_id, X.anime_enc_id), total=X.shape[0]):\n",
    "            if user_id in self.user_emb and anime_id in anime.index:\n",
    "                user_emb = self.user_emb[user_id]\n",
    "                anime_emb = anime.iloc[anime_id][4:]\n",
    "                cos_sim = np.dot(user_emb, anime_emb) / np.linalg.norm(anime_emb)\n",
    "                y_pred.append(cos_sim)\n",
    "            else:\n",
    "                y_pred.append(self.user_mean[user_id])\n",
    "\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "genre_sim_rating = GenresSimRating()\n",
    "genre_sim_rating.fit(train_ratings, train_ratings.rating)\n",
    "\n",
    "y_pred = genre_sim_rating.predict(test_ratings)\n",
    "\n",
    "mae = mean_absolute_error(test_ratings.rating, y_pred)\n",
    "\n",
    "add_res('Genre Similiraty Rating', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings\n",
    "\n",
    "BERT is a language model that can be used to get embeddings of text. We will use it to get embeddings of the synopsis of the anime. Then, we will use the embeddings to find the similarity between the animes. We will use the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def bert_embed_parallel(texts, batch_size=32):\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Process texts in batches\n",
    "    all_embeddings = []\n",
    "    for i in tqdm_notebook(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize and pad the batch\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Get embeddings with no gradient computation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Compute the mean of the last hidden states for each input in the batch\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Example usage\n",
    "texts = list(anime.synopsis.values)\n",
    "embedded_texts = bert_embed_parallel(texts)\n",
    "\n",
    "# save the embeddings\n",
    "np.save('bert_embeddings.npy', embedded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a similar model as with the genres, but instead of using the genres, we will use the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ––\n",
    "# THIS IS MOST INEFFICIENT WAY TO DO THIS\n",
    "# THINK MAYBE HOW TO PARALLELIZE THIS\n",
    "# BY CHANGING THE FOR LOOP TO LINEAR ALGEBRA\n",
    "# ––\n",
    "\n",
    "class BertSimRating:\n",
    "    def __init__(self):\n",
    "        self.user_emb = {}\n",
    "        self.user_mean = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Calculating user embeddings...\")\n",
    "        self.user_mean = X.groupby('user_enc_id')['rating'].mean()\n",
    "        \n",
    "        for user_id, group in tqdm_notebook(X.groupby('user_enc_id')):\n",
    "            selection = anime.anime_id.isin(group.anime_id)\n",
    "            if selection.sum() == 0:\n",
    "                continue\n",
    "            emb = embedded_texts[selection].mean(axis=0)\n",
    "            self.user_emb[user_id] = emb / np.linalg.norm(emb)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Predicting...\")\n",
    "        y_pred = []\n",
    "\n",
    "        for user_id, anime_id in tqdm_notebook(zip(X.user_enc_id, X.anime_enc_id), total=X.shape[0]):\n",
    "            if user_id in self.user_emb and anime_id in anime.index:\n",
    "                user_emb = self.user_emb[user_id]\n",
    "                anime_emb = embedded_texts[anime_id]\n",
    "                cos_sim = np.dot(user_emb, anime_emb) / np.linalg.norm(anime_emb)\n",
    "                y_pred.append(cos_sim)\n",
    "            else:\n",
    "                y_pred.append(self.user_mean[user_id])\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sim_rating = BertSimRating()\n",
    "bert_sim_rating.fit(train_ratings, train_ratings.rating)\n",
    "\n",
    "y_pred = bert_sim_rating.predict(test_ratings)\n",
    "\n",
    "mae = mean_absolute_error(test_ratings.rating, y_pred)\n",
    "\n",
    "add_res('BERT Similiraty Rating', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
